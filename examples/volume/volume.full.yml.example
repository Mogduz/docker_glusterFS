# === [DOC] Autogenerierte Inline-Dokumentation: volume.full.yml.example ===
# Datei: examples/volume/volume.full.yml.example
# Typ: YAML (Compose/Volumes/Beispiele).
# Zweck: Deklarative Konfiguration (Container-Stack, Volume-Spezifikationen).
# Hinweis: Kommentare erläutern Felder, sinnvolle Defaults und Sicherheitsüberlegungen.
# === [DOC-END] ===

# Vollständiges Beispiel für **ein** Volume in der YAML, mit allen vom Entrypoint unterstützten Feldern.
# Diese Datei zeigt die Struktur und mögliche Werte. Kommentare können entfernt werden.
#
# Unterstützte Schlüssel pro Volume-Eintrag:
#   name               (Pflicht)        — Name des Gluster-Volumes
#   replica            (optional)       — Replika-Faktor (integer). Änderung an bestehenden Volumes erfordert manuelle Migration.
#   transport          (optional)       — Transporttyp, i.d.R. 'tcp'
#   auth_allow         (optional)       — Komma-separierte Liste von CIDRs/IPs/Hostnames oder "*" für alle
#                                         Leerstring "" -> setzt auth.allow zurück (Reset)
#   nfs_disable        (optional,bool)  — true|false  -> wird zu nfs.disable on/off
#   options            (optional,map)   — Beliebige 'gluster volume set' Optionen als key: value
#   options_reset      (optional,CSV)   — Liste von Options-Namen, die mit 'gluster volume reset' zurückgesetzt werden
#   quota.limit        (optional)       — z.B. 200G, 1T; "off"/"disable"/"0" deaktiviert quota
#   quota.soft_limit_pct (optional,int) — z.B. 80 (setzt features.soft-limit auf 80%)
#
# Hinweise:
# - Brick-Zuordnung wird über Compose/.env erledigt. Der Entrypoint erzeugt pro Volume automatisch Unterordner brickX/<VOLNAME>.
# - Unbekannte Schlüssel werden geloggt und ignoriert.
# - 'replica' wird NICHT live umgestellt (erfordert add/remove-brick & Rebalance); Abweichungen werden geloggt.
# - Optionen sind idempotent: beim Start werden sie erneut gesetzt/gereset, falls nötig.
#
volumes:
  - name: gv0
    replica: 2               # optional (Standard im Solo-Modus)
    transport: tcp           # optional

    # Zugriffskontrolle:
    auth_allow: 10.0.1.0/24,192.168.1.0/24
    # Alternative: auf Default zurücksetzen -> auth_allow: ""


    # Gluster 'volume set' Optionen (Beispiele; nach Bedarf erweitern/ändern)

# Volume-Optionen (umfangreiche Auswahl; alle Schlüssel sind original Gluster-Keys)
# Hinweis:
# - Diese Datei dient als DOKU & Vorlage. Kommentierte Einträge sind Beispiele.
# - Prüfe die tatsächlich verfügbaren Optionen auf deinem System mit:
#     gluster volume get <VOL> all
# - Defaults können je nach Version/Build variieren (siehe Gluster-Doku).
options:
  # --- Zugriff/Authentisierung (Client-Zulassungen) ---
  auth.allow: "10.0.1.*"                 # Liste erlaubter Clients (CIDR/Host/IP; '*' = alle). Für Testnetze einschränken.
  # auth.reject: ""                       # Liste zu blockierender Clients. Leer lassen, wenn nicht nötig.

  # --- NFS (Legacy gNFS) ---
  nfs.disable: on                        # gNFS deaktivieren; empfohlen, wenn FUSE oder NFS-Ganesha genutzt wird.

  # --- Cluster/Quorum/Self-Heal (Replica-Volumes) ---
  cluster.quorum-type: auto              # auto/fixed/none. Für Replica-2 'auto' = Schreibsperre bei Netzsplit (Schutz vor Split-Brain).
  # cluster.quorum-count: 1              # nur bei quorum-type=fixed relevant.
  cluster.self-heal-daemon: on           # proaktives Self-Heal aktiv (Standard).
  cluster.heal-timeout: 10               # Sek.; schnellere Prüffrequenz für Self-Heal (Default 600). Aggressiver für Lab/Tests.
  cluster.background-self-heal-count: 8  # parallele Heal-Jobs pro Client.
  cluster.favorite-child-policy: "mtime" # automatische Split-Brain-Auflösung (mit Vorsicht einsetzen). Alternativen: size|ctime|majority
  cluster.lookup-unhashed: on            # Lookup über alle Subvolumes (spürbar bei verteilten Workloads).
  cluster.lookup-optimize: on            # -ve Lookup Optimierung.
  cluster.min-free-disk: "10%"           # benötigt freien Platz (Disk-Reserve pro Brick).
  cluster.min-free-inodes: "10%"         # Reserve bzgl. Inodes.
  cluster.rebal-throttle: normal         # normal|lazy|aggressive – Migrations-Parallelität beim Rebalance.
  cluster.ensure-durability: on          # Durability erhöhen (Metadaten/Journal).

  # --- Netzwerk ---
  network.ping-timeout: 10               # Sekunden; aggressiver als Default 42s für schnellere Erkennung von Ausfällen.
  network.frame-timeout: 1800            # Sekunden; Operation-Timeout (Default).

  # --- Performance/Caches/Threads ---
  performance.client-io-threads: on      # IO-Thread-Translator für Clients.
  performance.io-thread-count: 16        # I/O Threads (Default 16); auf schwächeren Knoten ggf. reduzieren.
  performance.quick-read: on             # Hot-Read Pfad.
  performance.io-cache: on               # Read-Cache.
  performance.cache-size: 128MB          # Read-Cache-Größe (Default 32MB). Für große sequentielle Reads ggf. erhöhen.
  performance.cache-min-file-size: 0B    # nur Dateien >= dieser Größe cachen.
  performance.cache-max-file-size: 2GB   # nur Dateien <= dieser Größe cachen.
  performance.cache-refresh-timeout: 1   # Sekunden bis Revalidierung.
  performance.md-cache: on               # Metadaten-Cache (beschleunigt ls/stat).
  performance.md-cache-timeout: 5        # Sekunden bis Metadaten-Revalidierung (Default 1).
  performance.open-behind: on            # Open erst bei Bedarf zum Backend.
  performance.lazy-open: yes             # setzt open-behind voraus (Yes/No).
  performance.stat-prefetch: on          # Stat-Vorabrufe.
  performance.readdir-ahead: on          # Directory-Read-Ahead; schnelleres Verzeichnis-Listing.
  performance.parallel-readdir: on       # parallele Verzeichnis-Liste (siehe Doku).
  performance.strict-write-ordering: on  # spätere Writes überholen frühere nicht.
  performance.strict-o-direct: on        # O_DIRECT wirkt wie erwartet (keine Write-Behind-Caches).
  performance.rda-cache-limit: 32MB      # Gesamtlimit für readdir-ahead Cache.
  performance.rda-request-size: 128KB    # Größe der Directory-Entry-Puffer.
  performance.write-behind: on           # Write-Behind aktivieren.
  performance.write-behind-window-size: 1MB # Puffergröße pro Inode.
  performance.flush-behind: on           # Flush asynchron zulassen.

  # --- Server/Transport ---
  # server.allow-insecure: off           # Nur aktivieren, wenn viele Bricks/Ports – siehe Doku.
  # server.statedump-path: /var/run/gluster # Pfad für Statedumps.

  # --- Storage/Umgang mit Dateien & Masken ---
  storage.reserve: "1%"                  # Reserviert Platz pro Brick (Default 1%). Für Testsysteme i. d. R. OK.
  storage.health-check-interval: 30      # Sek.; 0 zum Deaktivieren.
  storage.create-mask: "0777"            # max. Datei-Rechte (Upper bound).
  storage.create-directory-mask: "0777"  # max. Verzeichnis-Rechte.
  storage.force-create-mode: "0000"      # min. Datei-Rechte (Lower bound).
  storage.force-create-directory: "0000" # min. Verzeichnis-Rechte.

  # --- Features ---
  # features.trash: off                  # Papierkorb (löschen → Trash). Aktivieren, wenn sinnvoll.
  # features.shard-block-size: 64MB      # nur bei Sharding relevant.
  # features.uss: off                    # User Serviceable Snapshots.

  # --- Diagnostik/Logging ---
  diagnostics.brick-sys-log-level: INFO  # Brick-Log-Level.
